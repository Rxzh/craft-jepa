#--- VPT-JEPA Action-Conditioned Post-Training Config ---
#Based on the DROID AC config and V-JEPA ViT-G/16 model.
#Note: action_embed_dim must match your vpt_states_to_diffs output.
#--- SLURM / Infrastructure ---
# Environment variables (set in SLURM script or shell):
#   VJEPA_OUTPUT_DIR: Output directory for checkpoints/logs (default: $SCRATCH/vpt_jepa/vitg16-256px-8f)
#   VJEPA_DATA_PATH: WebDataset shard path pattern (default: $WORK/petrus/craft-jepa/VPT/shard-*.tar)
#   VJEPA_ACTION_SCALER_PATH: Path to action scaler pickle (default: $WORK/petrus/craft-jepa/my_scaler_400k.pkl)
#   VJEPA_PRETRAIN_CHECKPOINT: Path to pretrained V-JEPA checkpoint (default: $WORK/petrus/craft-jepa/weights/vitg.pt)
app: vpt_minecraft
cpus_per_task: 24
folder: ${VJEPA_OUTPUT_DIR}
mem_per_gpu: 220G
nodes: 1
tasks_per_node: 1
#--- Data Config ---
data:
  batch_size: 8
  crop_size: 256
  # Path to the pre-fitted action scaler (from action_scaler_fit_optimized.py)
  # This normalizes the 4 continuous action dimensions (mouse_dx, mouse_dy, yaw_diff, pitch_diff)
  action_scaler_path: ${VJEPA_ACTION_SCALER_PATH}

#Use a glob string for all your WebDataset shards
  datasets:
    # dataset paths
    - ${VJEPA_DATA_PATH}
  dataset_fpcs:
  - 16  # Corresponds to frames_per_clip
  fps: 5
  num_workers: 12
  patch_size: 16
  pin_mem: true
  tubelet_size: 2 # Corresponds to frameskip

#--- Data Augmentation Config ---
#Note: Your vpt_dataset.py must be updated to accept and use the transform object for these augs to take effect.
data_aug:
  auto_augment: false
  horizontal_flip: false # Minecraft UI should not be flipped
  motion_shift: false
  random_resize_aspect_ratio:
  - 0.75
  - 1.33 # Standard V-JEPA
  random_resize_scale:
  - 0.3 # Standard V-JEPA
  - 1.0
  reprob: 0.0

#--- Loss Config ---
loss:
  auto_steps: 2
  loss_exp: 1.0
  normalize_reps: true
  reg_coeff: 0.0

#--- Meta Config ---
meta:
  dtype: bfloat16
  eval_freq: 100
  load_predictor: false
  #--- CHECKPOINT ---
  #Path to your pretrained V-JEPA ViT-G/16 checkpoint
  pretrain_checkpoint: ${VJEPA_PRETRAIN_CHECKPOINT}
  context_encoder_key: target_encoder
  target_encoder_key: target_encoder
  #--- END CHECKPOINT ---
  resume_checkpoint: null
  save_every_freq: 25
  seed: 239
  use_sdpa: true

#--- Model Config ---
model:
  model_name: vit_giant_xformers # For V-JEPA ViT-G/16
  pred_depth: 24
  pred_embed_dim: 1024 # Predictor embed dim
  pred_is_frame_causal: true
  pred_num_heads: 16
  uniform_power: true
  use_activation_checkpointing: true
  #--- CRITICAL ---
  use_extrinsics: false # VPT data does not have this
  action_embed_dim: 38
  # Must match the output of vpt_dataset.py ProcessVPT:
  # 4 continuous (mouse_dx, mouse_dy, yaw_diff, pitch_diff)
  # + 21 keyboard keys (multi-hot)
  # + 3 mouse buttons (multi-hot)
  # + 9 hotbar slots (one-hot)
  # + 1 gui_open (binary)
  # = 38 total dimensions
  #--- END CRITICAL ---
  use_rope: true


#Optimization Config ---
optimization:
  anneal: 15
  epochs: 315
  final_lr: 0.0
  final_weight_decay: 0.04
  ipe: 300 # Iterations per epoch, tune based on dataset size
  lr: 0.000425
  start_lr: 0.000075
  warmup: 15
  weight_decay: 0.04