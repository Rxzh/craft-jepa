{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2193731b",
   "metadata": {},
   "source": [
    "Below are architectural directions that keep the **ViT‑g encoder frozen** and concentrate changes in the **predictor** and auxiliary modules so that (a) temporal compute is near‑linear, (b) a persistent world state can survive viewpoint changes, and (c) training does not destabilize when supervising hundreds/thousands of steps.\n",
    "\n",
    "The suggestions assume the encoder produces per‑frame latent tokens \\(z_t\\) (either all patch tokens or a pooled summary). In practice, long‑horizon modeling becomes much easier if each time step is reduced to a **small number of state tokens** (e.g., 1–8) rather than thousands of spatial tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) First “must-do” change: introduce an explicit temporal state bottleneck\n",
    "**Problem:** If the predictor attends over all past patch tokens, cost is $O(T^2 \\cdot N_\\text{tokens}^2)$ or worse. Long-range is dead on arrival.\n",
    "\n",
    "**Change:** Define a learned compression module $C(\\cdot)$ that maps the frozen ViT output to a small state:\n",
    "- $z_t^\\text{enc} = E(x_t)$ (frozen ViT; could be patch tokens)\n",
    "- $s_t = C(z_t^\\text{enc})$, where $s_t \\in \\mathbb{R}^{M \\times d}$ with small $M$ (1–8)\n",
    "\n",
    "Concrete $C$ options:\n",
    "1. **CLS-only** (fastest, weakest): take ViT CLS token as $s_t$.\n",
    "2. **Attention pooling / Perceiver bottleneck:** a small set of learned latent queries cross-attend to encoder tokens (Perceiver-style), outputting $M$ latent slots.\n",
    "3. **Spatial token downsampler:** lightweight pooling + MLP to produce $M$ tokens.\n",
    "\n",
    "This step alone usually dominates feasibility. Everything below assumes the temporal model operates on $s_t$ (plus memory), not raw patch grids.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6f7f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1) Temporal bottleneck beyond quadratic attention: three viable predictor families\n",
    "\n",
    "### A) Recurrent State Space Model (SSM) core (Mamba/S4-style)\n",
    "**Goal:** make temporal modeling $O(T)$ and allow thousands of steps.\n",
    "\n",
    "Replace the transformer predictor with a **causal recurrent SSM block** over time:\n",
    "- Input per step: $[s_t, a_t, r_t]$ where $a_t$ is action embedding and $r_t$ is retrieved memory (optional)\n",
    "- Hidden dynamics: $h_{t+1} = \\text{SSM}(h_t, [s_t, a_t, r_t])$\n",
    "- Prediction head: $\\hat{s}_{t+\\Delta} = g_\\Delta(h_t)$ for one or multiple horizons $\\Delta$\n",
    "\n",
    "**Why this fits JEPA:** JEPA only needs a predictive mapping in latent space; SSMs provide long context with linear scaling and stable inductive bias for long sequences.\n",
    "\n",
    "**Implementation details that matter**\n",
    "- Use **multi-rate** update: update fast state every step, update slow state every $K$ steps (see §3).\n",
    "- Treat actions as input forcing to the SSM (see §4).\n",
    "- For memory retrieval, feed retrieved tokens as additional inputs (do not concatenate entire history).\n",
    "\n",
    "### B) Linear/sparse attention with recurrence (Transformer‑XL / Longformer-like)\n",
    "If keeping transformer tooling is preferred:\n",
    "- Local window attention over recent steps (e.g., last 128)\n",
    "- Plus a **recurrent memory** of compressed past (Transformer‑XL segment-level recurrence)\n",
    "- Optionally add **top‑k sparse retrieval** from a memory bank (see §2)\n",
    "\n",
    "Compute becomes roughly $O(T \\cdot W)$ with window $W$, plus retrieval cost.\n",
    "\n",
    "This tends to be easier than full SSM replacement but is less compute-predictable and can still be heavy if $W$ must be large.\n",
    "\n",
    "### C) Hierarchical chunked temporal model (two-stage temporal aggregation)\n",
    "Split time into chunks of length $K$:\n",
    "- Within-chunk: small transformer/MLP/SSM to model short-term transitions\n",
    "- Across chunks: second model that updates a chunk-level state $S_c$\n",
    "\n",
    "This gives an explicit mechanism to preserve information over long horizons without storing all micro-steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a128cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2) Persistent latent memory: a “global latent buffer” that doesn’t explode compute\n",
    "Minecraft needs persistence under viewpoint changes. A pure reactive predictor will forget what is behind the agent unless some world memory is carried forward.\n",
    "\n",
    "### A) External memory with **sparse read/write** (key-value + top‑k retrieval)\n",
    "Maintain a memory bank $ \\mathcal{M} = \\{(k_i, v_i)\\}_{i=1}^N $:\n",
    "- **Keys** $k_i$: compact descriptors of state + pose context (e.g., pooled latent + predicted egomotion)\n",
    "- **Values** $v_i$: one or few latent slots to retrieve (could be $s_t$ or a learned “scene slot”)\n",
    "- **Read:** retrieve top‑k by similarity $ \\text{sim}(q_t, k_i)$ where $q_t$ comes from current $s_t$ and/or hidden state $h_t$\n",
    "- **Write:** add/update memory with gating (only write when novelty is high)\n",
    "\n",
    "This avoids attending over all past steps; compute is $O(k)$ per step.\n",
    "\n",
    "**Key design choice (important for Minecraft):** retrieval should be conditioned on **estimated pose** (see below) to support “turn 180° and remember behind”.\n",
    "\n",
    "### B) Compressive memory (Compressive Transformer idea)\n",
    "Keep two buffers:\n",
    "- short buffer: last $W$ steps (exact tokens)\n",
    "- long buffer: compressed summaries of older steps (e.g., via pooling/auto-compression)\n",
    "\n",
    "Read attends to short buffer densely and to long buffer sparsely.\n",
    "\n",
    "### C) Map-like memory (optional, higher effort, fits Minecraft well)\n",
    "If actions allow reasonable egomotion inference, maintain a latent “map” in a discrete grid (or hashed voxel map):\n",
    "- Use camera yaw/pitch + movement actions to update an egocentric pose estimate $\\hat{p}_t$\n",
    "- Write latent observations into map cells keyed by $\\hat{p}_t$\n",
    "- Read returns nearby cells in agent coordinates\n",
    "\n",
    "This becomes a learned SLAM-like module. It is often the difference between “forgets behind” and “persistent world”.\n",
    "\n",
    "**Skeptical note:** without some notion of pose (even learned), memory retrieval tends to become content-addressed only, which is brittle in Minecraft because many regions share textures (trees, stone) and the agent needs geometry/position continuity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b30d6",
   "metadata": {},
   "source": [
    "## 3) Hierarchical JEPA: two-tier predictor for “skills/subgoals” + “frames”\n",
    "A practical long-range approach is to predict at **multiple temporal resolutions** and force consistency.\n",
    "\n",
    "### Structure\n",
    "- **Fast predictor** $F$: predicts next-step (or short horizon) latent transitions\n",
    "  $$\n",
    "  \\hat{s}_{t+1} = F(s_t, a_t, m_t)\n",
    "  $$\n",
    "- **Slow predictor** $G$: predicts a coarse “macro-state” every $K$ steps:\n",
    "  $$\n",
    "  \\hat{u}_{c+1} = G(u_c, A_{c:c+K-1}, M_c)\n",
    "  $$\n",
    "  where $u_c$ is a slow state and $A$ summarizes actions over the chunk.\n",
    "\n",
    "Then condition the fast model on the slow state $u$ (FiLM/gating or cross-attention):\n",
    "$$\n",
    "\\hat{s}_{t+1} = F(s_t, a_t, m_t; u_{\\lfloor t/K \\rfloor})\n",
    "$$\n",
    "\n",
    "### Targets for the slow level\n",
    "Two common choices:\n",
    "1. **Downsampled encoder latents:** define $u_c = \\text{pool}(s_{cK : cK+K-1})$\n",
    "2. **Separate learned slow bottleneck:** $u_c = C_\\text{slow}(z_{cK}^\\text{enc})$\n",
    "\n",
    "Hierarchical supervision is useful because long-horizon planning often needs stable, slowly varying variables (location, inventory, current build goal) that aren’t captured by one-step dynamics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f5a4a",
   "metadata": {},
   "source": [
    "## 4) Action integration for long-range influence (30s+)\n",
    "Minecraft actions are hybrid (discrete buttons + continuous camera deltas). The key is to avoid representing actions as a single token that only affects the next step.\n",
    "\n",
    "### Recommended action representation\n",
    "Factorize $a_t$ into semantically distinct components and embed each:\n",
    "- Movement keys (forward/back/left/right/jump/sneak/sprint): discrete embedding\n",
    "- Interaction (attack/use): discrete embedding\n",
    "- Hotbar slot / inventory actions: discrete embedding\n",
    "- Camera $(\\Delta \\text{yaw}, \\Delta \\text{pitch})$: continuous projected via MLP + clipping + optionally sinusoidal features\n",
    "\n",
    "Then combine via concatenation + MLP to $e_t$.\n",
    "\n",
    "### How actions influence long horizons\n",
    "Use one (or combine):\n",
    "1. **Recurrent dynamics injection:** in SSM/RNN-like cores, actions naturally influence the hidden state over long time via recurrence.\n",
    "2. **Chunked action summarizer for the slow tier:** for slow predictor $G$, summarize action sequences over $K$ steps using a small SSM/transformer:\n",
    "$$\n",
    "\\bar{A}_c = \\text{Summarize}(a_{cK:cK+K-1})\n",
    "$$\n",
    "3. **Action-conditioned gating (FiLM):** produce per-layer scale/shift for predictor blocks from action embedding, allowing persistent modulation.\n",
    "\n",
    "A practical issue: camera deltas dominate pixel change but are not “world change.” It helps to let the model learn a separation between **egomotion** and **world state update**, e.g. by predicting pose internally (even self-supervised) and conditioning memory reads on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f0f6b",
   "metadata": {},
   "source": [
    "## 5) Training objectives for long rollouts without collapse / drift\n",
    "Long-horizon JEPA training tends to fail via (i) compounding error and (ii) trivial low-variance representations over rollout. The objective usually needs to change from “predict 1 step” to “predict many steps under rollout”.\n",
    "\n",
    "### A) Multi-horizon JEPA loss (necessary)\n",
    "For multiple horizons $\\Delta \\in \\{1,2,4,8,\\dots,H\\}$:\n",
    "- Target: $s_{t+\\Delta}^\\star = \\text{sg}(C(E(x_{t+\\Delta})))$ with EMA target encoder and stop-grad\n",
    "- Prediction: $\\hat{s}_{t+\\Delta} = P_\\Delta(\\text{context up to }t)$\n",
    "\n",
    "Loss:\n",
    "$$\n",
    "\\mathcal{L}_\\text{JEPA} = \\sum_{\\Delta} w_\\Delta \\, d(\\hat{s}_{t+\\Delta}, s_{t+\\Delta}^\\star)\n",
    "$$\n",
    "with $d$ = cosine distance or smooth L2 in a normalized space.\n",
    "\n",
    "Use a **curriculum** on $H$ (increase max horizon gradually), otherwise optimization often collapses early.\n",
    "\n",
    "### B) Rollout (free-running) consistency loss (strongly recommended)\n",
    "Train the predictor not only with teacher-forcing context, but by unrolling on its own predictions:\n",
    "- Rollout: $\\tilde{s}_{t+1} = F(\\tilde{s}_t, a_t, \\tilde{m}_t)$ starting from true $s_t$\n",
    "- Compare $\\tilde{s}_{t+k}$ to target $s_{t+k}^\\star$\n",
    "\n",
    "This addresses compounding error. Truncated BPTT is fine (e.g., unroll 32–128, but supervise sparse long horizons too).\n",
    "\n",
    "### C) Anti-collapse regularization in latent space\n",
    "JEPA/BYOL-style setups can collapse if the predictor finds a constant solution and the target stops providing diversity (especially when multi-step errors grow).\n",
    "\n",
    "Add one of:\n",
    "- **VICReg-style variance/covariance regularizers** on predicted states across batch/time:\n",
    "  - encourage per-dimension variance above a threshold\n",
    "  - penalize off-diagonal covariance\n",
    "- **Feature normalization + predictor MLP** (already common) but for long horizon, explicit variance/cov often helps.\n",
    "- Optional: occasional **contrastive negatives** (InfoNCE) at the state-token level if collapse is observed in practice.\n",
    "\n",
    "### D) Memory-specific auxiliary losses (to make memory actually used)\n",
    "Memory modules often get ignored unless forced.\n",
    "Useful auxiliary objectives:\n",
    "1. **Viewpoint persistence:** when the agent turns away and later returns, enforce that memory helps predict the re-observed latent.\n",
    "2. **Loop closure / retrieval accuracy:** given query $q_t$, retrieved memory should match the latent of the corresponding past observation (self-supervised via temporal proximity + pose estimate).\n",
    "3. **Reconstruction-in-latent of occluded content:** mask some state slots and require memory retrieval to fill them (JEPA-style masked prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ad3a8",
   "metadata": {},
   "source": [
    "## 6) Putting it together: one concrete blueprint that is likely to work\n",
    "A high-probability, engineering-feasible design:\n",
    "\n",
    "1. **Frozen encoder $E$** (ViT‑g) → produces patch tokens per frame.\n",
    "2. **Perceiver-style bottleneck $C$** → produces $M=4$ state slots $s_t$.\n",
    "3. **External memory $ \\mathcal{M}$** with top‑k retrieval:\n",
    "   - keys: $k_t = \\text{MLP}([s_t, \\hat{p}_t])$\n",
    "   - values: $v_t = s_t$ (or a compressed variant)\n",
    "   - query: from predictor hidden state\n",
    "4. **Temporal core:** Mamba/SSM over time producing hidden $h_t$, conditioned on action embedding and retrieved memory $r_t$.\n",
    "5. **Two-tier hierarchy:**\n",
    "   - slow SSM updates $u_c$ every $K$ steps using summarized actions\n",
    "   - fast SSM predicts next-step $s_{t+1}$ conditioned on $u$\n",
    "6. **Loss:**\n",
    "   - multi-horizon JEPA over $\\Delta\\in\\{1,2,4,\\dots,1024\\}$ (not all at once; sample a subset)\n",
    "   - rollout consistency for unrolled predictions\n",
    "   - VICReg regularization on predicted $s$ (and optionally on $u$)\n",
    "   - memory auxiliary retrieval/persistence loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85f4ec",
   "metadata": {},
   "source": [
    "## 7) Two practical warnings (based on failure modes seen in long-horizon latent models)\n",
    "1. **If $s_t$ is too information-poor**, the predictor will “average out” the world and long-horizon prediction becomes trivial-but-useless. That is why $M>1$ latent slots and/or a slow state $u$ often matters.\n",
    "2. **If memory addressing is purely content-based**, Minecraft’s repeated textures cause false matches. Some pose/egomotion signal (even learned) usually becomes necessary for persistent world state.\n",
    "\n",
    "---\n",
    "\n",
    "If you share (a) what V‑JEPA 2 exposes as the latent target (CLS vs patch tokens vs pooled) and (b) your intended training sequence length and batch constraints, it is possible to propose a more specific module sizing (number of slots $M$, memory size, top‑k, chunk size $K$, and which horizons $\\Delta$ to sample) that fits a realistic compute budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b77d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
